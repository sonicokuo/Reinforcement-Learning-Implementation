# -*- coding: utf-8 -*-
"""RLHW0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14xL6p1In7MrCh02RJMUKg9kvBcryq6UM
"""

# Spring 2023, 535515 Reinforcement Learning
# HW0: Policy Iteration and Value iteration for MDPs
       
import numpy as np
import gym

def get_rewards_and_transitions_from_env(env):
    # Get state and action space sizes
    num_states = env.observation_space.n
    num_actions = env.action_space.n

    # Intiailize matrices
    R = np.zeros((num_states, num_actions, num_states))
    P = np.zeros((num_states, num_actions, num_states))

    # Get rewards and transition probabilitites for all transitions from an OpenAI gym environment
    for s in range(num_states):
        for a in range(num_actions):
            for transition in env.P[s][a]:
                prob, s_, r, done = transition
                R[s, a, s_] = r
                P[s, a, s_] = prob
                
    return R, P

def value_iteration(env, gamma=0.9, max_iterations=10**6, eps=10**-3):
    """        
        Run value iteration (You probably need no more than 30 lines)
        
        Input Arguments
        ----------
            env: 
                the target environment
            gamma: float
                the discount factor for rewards
            max_iterations: int
                maximum number of iterations for value iteration
            eps: float
                for the termination criterion of value iteration 
        ----------
        
        Output
        ----------
            policy: np.array of size (500,)
        ----------
        
        TODOs
        ----------
            1. Initialize the value function V(s)
            2. Get transition probabilities and reward function from the gym env
            3. Iterate and improve V(s) using the Bellman optimality operator
            4. Derive the optimal policy using V(s)
        ----------
    """
    num_spaces = env.observation_space.n
    num_actions = env.action_space.n
    
    # Initialize with a random policy
    policy = np.array([env.action_space.sample() for _ in range(num_spaces)])
    
    ##### FINISH TODOS HERE #####
    
    #initialize the original state-value fuction and get transition probabilities and reward function from the gym env
    x = get_rewards_and_transitions_from_env(env)
    value = np.zeros(num_spaces)
    

    for i in range(max_iterations):
      new_value = np.zeros(num_spaces)
      for j in range(num_spaces):
        highest_reward = 0
        #calculate the new expected values for each state and possible action in different states
        for k in range(num_actions):
          expected_reward = x[0][j, k].dot(x[1][j, k])
          future_reward = x[1][j, k].dot(value)
          total_reward = expected_reward + gamma * future_reward
          if total_reward > highest_reward:
            highest_reward = total_reward
        new_value[j] = highest_reward

      # if the difference between new and old value function is less than eps, stop updating the value function
      diff = sum([abs(x-y) for x, y in zip(value.flatten(), new_value.flatten())])        
      if diff<eps:
        break
      value = new_value

    #Generate the conrreponding policy based on the final value funciton
    for j in range(num_spaces):
      highest_reward = 0
      highest_action = 0
      for k in range(num_actions):
        expected_rewarad = 0
        future_reward = 0
        expected_reward = x[0][j, k].dot(x[1][j, k])

        future_reward = x[1][j, k].dot(value)
        total_reward = expected_reward + gamma * future_reward
        if total_reward > highest_reward:
          highest_reward = total_reward
          highest_action = k
      policy[j] = highest_action
                  
    #############################
    
    # Return optimal policy    
    return policy

def policy_iteration(env, gamma=0.9, max_iterations=10**6, eps=10**-3):
    """ 
        Run policy iteration (You probably need no more than 30 lines)
        
        Input Arguments
        ----------
            env: 
                the target environment
            gamma: float
                the discount factor for rewards
            max_iterations: int
                maximum number of iterations for the policy evalaution in policy iteration
            eps: float
                for the termination criterion of policy evaluation 
        ----------  
        
        Output
        ----------
            policy: np.array of size (500,)
        ----------
        
        TODOs
        ----------
            1. Initialize with a random policy and initial value function
            2. Get transition probabilities and reward function from the gym env
            3. Iterate and improve the policy
        ----------
    """
    num_spaces = env.observation_space.n
    num_actions = env.action_space.n
    
    # Initialize with a random policy
    policy = np.array([env.action_space.sample() for _ in range(num_spaces)])
    
    ##### FINISH TODOS HERE #####

    x = get_rewards_and_transitions_from_env(env)


    for i in range(max_iterations):
      new_policy = np.array([env.action_space.sample() for _ in range(num_spaces)])
      current_value = np.zeros(num_spaces)
      #Do policy evaluation for current policy
      for policy_evaluation in range(max_iterations):
        new_value = np.zeros(num_spaces)
        for j in range(num_spaces):
          expected_reward = x[0][j, policy[j]].dot(x[1][j, policy[j]])
          future_reward = gamma * x[1][j, policy[j]].dot(current_value)
          new_value[j] = expected_reward + future_reward
        diff = sum([abs(x-y) for x, y in zip(current_value.flatten(), new_value.flatten())])        
        if diff < eps:
          break
        current_value = new_value
      #Update the policy based on the produced value fucntion
      for j in range(num_spaces):
        highest_reward = 0
        highest_action = 0
        for k in range(num_actions):        
          expected_reward = x[0][j, k].dot(x[1][j, k])
          future_reward = gamma * x[1][j, k].dot(current_value)
          total_reward = expected_reward + future_reward
          # print(total_reward)
          if total_reward > highest_reward:
            highest_reward = total_reward
            highest_action = k
        #choose the bset action with the highest Q value
        new_policy[j] = highest_action
      if np.array_equal(policy, new_policy):
        break
      policy = new_policy
    


    #############################

    # Return optimal policy
    
    return policy

def print_policy(policy, mapping=None, shape=(0,)):
    print(np.array([mapping[action] for action in policy]).reshape(shape))


def run_pi_and_vi(env_name):
    """ 
        Enforce policy iteration and value iteration
    """    
    env = gym.make(env_name)
    print('== {} =='.format(env_name))
    print('# of actions:', env.action_space.n)
    print('# of states:', env.observation_space.n)
    print(env.desc)

    vi_policy = value_iteration(env)
    pi_policy = policy_iteration(env)

    return pi_policy, vi_policy



if __name__ == '__main__':
    # OpenAI gym environment: Taxi-v2 or Taxi-v3
    pi_policy, vi_policy = run_pi_and_vi('Taxi-v3')


    # For debugging
    action_map = {0: "S", 1: "N", 2: "E", 3: "W", 4: "P", 5: "D"}
    print_policy(pi_policy, action_map, shape=None)
    print_policy(vi_policy, action_map, shape=None)
    
    # Compare the policies obatined via policy iteration and value iteration
    diff = sum([abs(x-y) for x, y in zip(pi_policy.flatten(), vi_policy.flatten())])        
    print('Discrepancy:', diff)